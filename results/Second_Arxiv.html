<!DOCTYPE html>
<html>
  <head>
    <title>Literature Research Arxiv</title>
    <link href="../utils/style.css" rel="stylesheet">
  </head>
  <body>
    <body>
      <div id="content">
        <a id="Arxiv"></a>
        <table style="width:1400px">
          <tbody>
            <tr>
              <th style="background-color:#c6f2b3">Title</th>
              <th style="background-color:#c6f2b3">Summary</th>
              <th style="background-color:#c6f2b3">Download</th>
            </tr>
            <tr>
              <td>Deep-CNN based Robotic Multi-Class Under-Canopy Weed Control in Precision Farming
                <td>Smart weeding systems to perform plant-specific operations can contribute to
the sustainability of agriculture and the environment. Despite monumental
advances in autonomous robotic technologies for precision weed management in
recent years, work on under-canopy weeding in fields is yet to be realized. A
prerequisite of such systems is reliable detection and classification of weeds
to avoid mistakenly spraying and, thus, damaging the surrounding plants.
Real-time multi-class weed identification enables species-specific treatment of
weeds and significantly reduces the amount of herbicide use. Here, our first
contribution is the first adequately large realistic image dataset
\textit{AIWeeds} (one/multiple kinds of weeds in one image), a library of about
10,000 annotated images of flax, and the 14 most common weeds in fields and
gardens taken from 20 different locations in North Dakota, California, and
Central China. Second, we provide a full pipeline from model training with
maximum efficiency to deploying the TensorRT-optimized model onto a single
board computer. Based on \textit{AIWeeds} and the pipeline, we present a
baseline for classification performance using five benchmark CNN models. Among
them, MobileNetV2, with both the shortest inference time and lowest memory
consumption, is the qualified candidate for real-time applications. Finally, we
deploy MobileNetV2 onto our own compact autonomous robot \textit{SAMBot} for
real-time weed detection. The 90\% test accuracy realized in previously unseen
scenes in flax fields (with a row spacing of 0.2-0.3 m), with crops and weeds,
distortion, blur, and shadows, is a milestone towards precision weed control in
the real world. We have publicly released the dataset and code to generate the
results at
\url{https://github.com/StructuresComp/Multi-class-Weed-Classification}.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2112.13986v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Fully Convolutional Networks with Sequential Information for Robust Crop and Weed Detection in Precision Farming
                <td>Reducing the use of agrochemicals is an important component towards
sustainable agriculture. Robots that can perform targeted weed control offer
the potential to contribute to this goal, for example, through specialized
weeding actions such as selective spraying or mechanical weed removal. A
prerequisite of such systems is a reliable and robust plant classification
system that is able to distinguish crop and weed in the field. A major
challenge in this context is the fact that different fields show a large
variability. Thus, classification systems have to robustly cope with
substantial environmental changes with respect to weed pressure and weed types,
growth stages of the crop, visual appearance, and soil conditions. In this
paper, we propose a novel crop-weed classification system that relies on a
fully convolutional network with an encoder-decoder structure and incorporates
spatial information by considering image sequences. Exploiting the crop
arrangement information that is observable from the image sequences enables our
system to robustly estimate a pixel-wise labeling of the images into crop and
weed, i.e., a semantic segmentation. We provide a thorough experimental
evaluation, which shows that our system generalizes well to previously unseen
fields under varying environmental conditions --- a key capability to actually
use such systems in precision framing. We provide comparisons to other
state-of-the-art approaches and show that our system substantially improves the
accuracy of crop-weed classification without requiring a retraining of the
model.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1806.03412v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>A Low-cost Robot with Autonomous Recharge and Navigation for Weed Control in Fields with Narrow Row Spacing
                <td>Modern herbicide application in agricultural settings typically relies on
either large scale sprayers that dispense herbicide over crops and weeds alike
or portable sprayers that require labor intensive manual operation. The former
method results in overuse of herbicide and reduction in crop yield while the
latter is often untenable in large scale operations. This paper presents the
first fully autonomous robot for weed management for row crops capable of
computer vision based navigation, weed detection, complete field coverage, and
automatic recharge for under \$400. The target application is autonomous
inter-row weed control in crop fields, e.g. flax and canola, where the spacing
between croplines is as small as one foot. The proposed robot is small enough
to pass between croplines at all stages of plant growth while detecting weeds
and spraying herbicide. A recharging system incorporates newly designed robotic
hardware, a ramp, a robotic charging arm, and a mobile charging station. An
integrated vision algorithm is employed to assist with charger alignment
effectively. Combined, they enable the robot to work continuously in the field
without access to electricity. In addition, a color-based contour algorithm
combined with preprocessing techniques is applied for robust navigation relying
on the input from the onboard monocular camera. Incorporating such compact
robots into farms could help automate weed control, even during late stages of
growth, and reduce herbicide use by targeting weeds with precision. The robotic
platform is field-tested in the flaxseed fields of North Dakota.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2112.02162v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Joint Stem Detection and Crop-Weed Classification for Plant-specific Treatment in Precision Farming
                <td>Applying agrochemicals is the default procedure for conventional weed control
in crop production, but has negative impacts on the environment. Robots have
the potential to treat every plant in the field individually and thus can
reduce the required use of such chemicals. To achieve that, robots need the
ability to identify crops and weeds in the field and must additionally select
effective treatments. While certain types of weed can be treated mechanically,
other types need to be treated by (selective) spraying. In this paper, we
present an approach that provides the necessary information for effective
plant-specific treatment. It outputs the stem location for weeds, which allows
for mechanical treatments, and the covered area of the weed for selective
spraying. Our approach uses an end-to-end trainable fully convolutional network
that simultaneously estimates stem positions as well as the covered area of
crops and weeds. It jointly learns the class-wise stem detection and the
pixel-wise semantic segmentation. Experimental evaluations on different
real-world datasets show that our approach is able to reliably solve this
problem. Compared to state-of-the-art approaches, our approach not only
substantially improves the stem detection accuracy, i.e., distinguishing crop
and weed stems, but also provides an improvement in the semantic segmentation
performance.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1806.03413v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Weed Density and Distribution Estimation for Precision Agriculture using Semi-Supervised Learning
                <td>Uncontrolled growth of weeds can severely affect the crop yield and quality.
Unrestricted use of herbicide for weed removal alters biodiversity and cause
environmental pollution. Instead, identifying weed-infested regions can aid
selective chemical treatment of these regions. Advances in analyzing farm
images have resulted in solutions to identify weed plants. However, a majority
of these approaches are based on supervised learning methods which requires
huge amount of manually annotated images. As a result, these supervised
approaches are economically infeasible for the individual farmer because of the
wide variety of plant species being cultivated. In this paper, we propose a
deep learning-based semi-supervised approach for robust estimation of weed
density and distribution across farmlands using only limited color images
acquired from autonomous robots. This weed density and distribution can be
useful in a site-specific weed management system for selective treatment of
infected areas using autonomous robots. In this work, the foreground vegetation
pixels containing crops and weeds are first identified using a Convolutional
Neural Network (CNN) based unsupervised segmentation. Subsequently, the weed
infected regions are identified using a fine-tuned CNN, eliminating the need
for designing hand-crafted features. The approach is validated on two datasets
of different crop/weed species (1) Crop Weed Field Image Dataset (CWFID), which
consists of carrot plant images and the (2) Sugar Beets dataset. The proposed
method is able to localize weed-infested regions a maximum recall of 0.99 and
estimate weed density with a maximum accuracy of 82.13%. Hence, the proposed
approach is shown to generalize to different plant species without the need for
extensive labeled data.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2011.02193v2">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Real-time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs
                <td>Precision farming robots, which target to reduce the amount of herbicides
that need to be brought out in the fields, must have the ability to identify
crops and weeds in real time to trigger weeding actions. In this paper, we
address the problem of CNN-based semantic segmentation of crop fields
separating sugar beet plants, weeds, and background solely based on RGB data.
We propose a CNN that exploits existing vegetation indexes and provides a
classification in real time. Furthermore, it can be effectively re-trained to
so far unseen fields with a comparably small amount of training data. We
implemented and thoroughly evaluated our system on a real agricultural robot
operating in different fields in Germany and Switzerland. The results show that
our system generalizes well, can operate at around 20Hz, and is suitable for
online operation in the fields.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1709.06764v2">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Geometrical Stem Detection from Image Data for Precision Agriculture
                <td>High efficiency in precision farming depends on accurate tools to perform
weed detection and mapping of crops. This allows for precise removal of harmful
weeds with a lower amount of pesticides, as well as increase of the harvest's
yield by providing the farmer with valuable information. In this paper, we
address the problem of fully automatic stem detection from image data for this
purpose. Our approach runs on mobile agricultural robots taking RGB images.
After processing the images to obtain a vegetation mask, our approach separates
each plant into its individual leaves and later estimates a precise stem
position. This allows an upstream mapping algorithm to add the high-resolution
stem positions as a semantic aggregate to the global map of the robot, which
can be used for weeding and for analyzing crop statistics. We implemented our
approach and thoroughly tested it on three different datasets with vegetation
masks and stem position ground truth. The experiments presented in this paper
conclude that our module is able to detect leaves and estimate the stem's
position at a rate of 56 Hz on a single CPU. We furthermore provide the
software to the community.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1812.05415v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming
                <td>An effective perception system is a fundamental component for farming robots,
as it enables them to properly perceive the surrounding environment and to
carry out targeted operations. The most recent methods make use of
state-of-the-art machine learning techniques to learn a valid model for the
target task. However, those techniques need a large amount of labeled data for
training. A recent approach to deal with this issue is data augmentation
through Generative Adversarial Networks (GANs), where entire synthetic scenes
are added to the training data, thus enlarging and diversifying their
informative content. In this work, we propose an alternative solution with
respect to the common data augmentation methods, applying it to the fundamental
problem of crop/weed segmentation in precision farming. Starting from real
images, we create semi-artificial samples by replacing the most relevant object
classes (i.e., crop and weeds) with their synthesized counterparts. To do that,
we employ a conditional GAN (cGAN), where the generative model is trained by
conditioning the shape of the generated object. Moreover, in addition to RGB
data, we take into account also near-infrared (NIR) information, generating
four channel multi-spectral synthetic images. Quantitative experiments, carried
out on three publicly available datasets, show that (i) our model is capable of
generating realistic multi-spectral images of plants and (ii) the usage of such
synthetic images in the training process improves the segmentation performance
of state-of-the-art semantic segmentation convolutional networks.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2009.05750v2">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Performance Evaluation of Deep Transfer Learning on Multiclass Identification of Common Weed Species in Cotton Production Systems
                <td>Precision weed management offers a promising solution for sustainable
cropping systems through the use of chemical-reduced/non-chemical robotic
weeding techniques, which apply suitable control tactics to individual weeds.
Therefore, accurate identification of weed species plays a crucial role in such
systems to enable precise, individualized weed treatment. This paper makes a
first comprehensive evaluation of deep transfer learning (DTL) for identifying
common weeds specific to cotton production systems in southern United States. A
new dataset for weed identification was created, consisting of 5187 color
images of 15 weed classes collected under natural lighting conditions and at
varied weed growth stages, in cotton fields during the 2020 and 2021 field
seasons. We evaluated 27 state-of-the-art deep learning models through transfer
learning and established an extensive benchmark for the considered weed
identification task. DTL achieved high classification accuracy of F1 scores
exceeding 95%, requiring reasonably short training time (less than 2.5 hours)
across models. ResNet101 achieved the best F1-score of 99.1% whereas 14 out of
the 27 models achieved F1 scores exceeding 98.0%. However, the performance on
minority weed classes with few training samples was less satisfactory for
models trained with a conventional, unweighted cross entropy loss function. To
address this issue, a weighted cross entropy loss function was adopted, which
achieved substantially improved accuracies for minority weed classes.
Furthermore, a deep learning-based cosine similarity metrics was employed to
analyze the similarity among weed classes, assisting in the interpretation of
classifications. Both the codes for model benchmarking and the weed dataset are
made publicly available, which expect to be be a valuable resource for future
research in weed identification and beyond.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2110.04960v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>A Rapidly Deployable Classification System using Visual Data for the Application of Precision Weed Management
                <td>In this work we demonstrate a rapidly deployable weed classification system
that uses visual data to enable autonomous precision weeding without making
prior assumptions about which weed species are present in a given field.
Previous work in this area relies on having prior knowledge of the weed species
present in the field. This assumption cannot always hold true for every field,
and thus limits the use of weed classification systems based on this
assumption. In this work, we obviate this assumption and introduce a rapidly
deployable approach able to operate on any field without any weed species
assumptions prior to deployment. We present a three stage pipeline for the
implementation of our weed classification system consisting of initial field
surveillance, offline processing and selective labelling, and automated
precision weeding. The key characteristic of our approach is the combination of
plant clustering and selective labelling which is what enables our system to
operate without prior weed species knowledge. Testing using field data we are
able to label 12.3 times fewer images than traditional full labelling whilst
reducing classification accuracy by only 14%.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1801.08613v2">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass Turf Using Inaccurate and Insufficient Training Data
                <td>To enable robotic weed control, we develop algorithms to detect nutsedge weed
from bermudagrass turf. Due to the similarity between the weed and the
background turf, manual data labeling is expensive and error-prone.
Consequently, directly applying deep learning methods for object detection
cannot generate satisfactory results. Building on an instance detection
approach (i.e. Mask R-CNN), we combine synthetic data with raw data to train
the network. We propose an algorithm to generate high fidelity synthetic data,
adopting different levels of annotations to reduce labeling cost. Moreover, we
construct a nutsedge skeleton-based probabilistic map (NSPM) as the neural
network input to reduce the reliance on pixel-wise precise labeling. We also
modify loss function from cross entropy to Kullback-Leibler divergence which
accommodates uncertainty in the labeling process. We implement the proposed
algorithm and compare it with both Faster R-CNN and Mask R-CNN. The results
show that our design can effectively overcome the impact of imprecise and
insufficient training sample issues and significantly outperform the Faster
R-CNN counterpart with a false negative rate of only 0.4%. In particular, our
approach also reduces labeling time by 95% while achieving better performance
if comparing with the original Mask R-CNN approach.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2106.08897v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Towards practical object detection for weed spraying in precision agriculture
                <td>The evolution of smaller, faster processors and cheaper digital storage
mechanisms across the last 4-5 decades has vastly increased the opportunity to
integrate intelligent technologies in a wide range of practical environments to
address a broad spectrum of tasks. One exciting application domain for such
technologies is precision agriculture, where the ability to integrate on-board
machine vision with data-driven actuation means that farmers can make decisions
about crop care and harvesting at the level of the individual plant rather than
the whole field. This makes sense both economically and environmentally.
However, the key driver for this capability is fast and robust machine vision
-- typically driven by machine learning (ML) solutions and dependent on
accurate modelling. One critical challenge is that the bulk of ML-based vision
research considers only metrics that evaluate the accuracy of object detection
and do not assess practical factors. This paper introduces three metrics that
highlight different aspects relevant for real-world deployment of precision
weeding and demonstrates their utility through experimental results.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2109.11048v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Extracting Pasture Phenotype and Biomass Percentages using Weakly Supervised Multi-target Deep Learning on a Small Dataset
                <td>The dairy industry uses clover and grass as fodder for cows. Accurate
estimation of grass and clover biomass yield enables smart decisions in
optimizing fertilization and seeding density, resulting in increased
productivity and positive environmental impact. Grass and clover are usually
planted together, since clover is a nitrogen-fixing plant that brings nutrients
to the soil. Adjusting the right percentages of clover and grass in a field
reduces the need for external fertilization. Existing approaches for estimating
the grass-clover composition of a field are expensive and time consuming -
random samples of the pasture are clipped and then the components are
physically separated to weigh and calculate percentages of dry grass, clover
and weeds in each sample. There is growing interest in developing novel deep
learning based approaches to non-destructively extract pasture phenotype
indicators and biomass yield predictions of different plant species from
agricultural imagery collected from the field. Providing these indicators and
predictions from images alone remains a significant challenge. Heavy occlusions
in the dense mixture of grass, clover and weeds make it difficult to estimate
each component accurately. Moreover, although supervised deep learning models
perform well with large datasets, it is tedious to acquire large and diverse
collections of field images with precise ground truth for different biomass
yields. In this paper, we demonstrate that applying data augmentation and
transfer learning is effective in predicting multi-target biomass percentages
of different plant species, even with a small training dataset. The scheme
proposed in this paper used a training set of only 261 images and provided
predictions of biomass percentages of grass, clover, white clover, red clover,
and weeds with mean absolute error of 6.77%, 6.92%, 6.21%, 6.89%, and 4.80%
respectively.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/2101.03198v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>Comparison of object detection methods for crop damage assessment using deep learning
                <td>Severe weather events can cause large financial losses to farmers. Detailed
information on the location and severity of damage will assist farmers,
insurance companies, and disaster response agencies in making wise post-damage
decisions. The goal of this study was a proof-of-concept to detect damaged crop
areas from aerial imagery using computer vision and deep learning techniques. A
specific objective was to compare existing object detection algorithms to
determine which was best suited for crop damage detection. Two modes of crop
damage common in maize (corn) production were simulated: stalk lodging at the
lowest ear and stalk lodging at ground level. Simulated damage was used to
create a training and analysis data set. An unmanned aerial system (UAS)
equipped with a RGB camera was used for image acquisition. Three popular object
detectors (Faster R-CNN, YOLOv2, and RetinaNet) were assessed for their ability
to detect damaged regions in a field. Average precision was used to compare
object detectors. YOLOv2 and RetinaNet were able to detect crop damage across
multiple late-season growth stages. Faster R-CNN was not successful as the
other two advanced detectors. Detecting crop damage at later growth stages was
more difficult for all tested object detectors. Weed pressure in simulated
damage plots and increased target density added additional complexity.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1912.13199v3">Download</a>
                  </td>
                </td>
              </td>
            </tr>
            <tr>
              <td>From Plants to Landmarks: Time-invariant Plant Localization that uses Deep Pose Regression in Agricultural Fields
                <td>Agricultural robots are expected to increase yields in a sustainable way and
automate precision tasks, such as weeding and plant monitoring. At the same
time, they move in a continuously changing, semi-structured field environment,
in which features can hardly be found and reproduced at a later time.
Challenges for Lidar and visual detection systems stem from the fact that
plants can be very small, overlapping and have a steadily changing appearance.
Therefore, a popular way to localize vehicles with high accuracy is based on
ex- pensive global navigation satellite systems and not on natural landmarks.
The contribution of this work is a novel image- based plant localization
technique that uses the time-invariant stem emerging point as a reference. Our
approach is based on a fully convolutional neural network that learns landmark
localization from RGB and NIR image input in an end-to-end manner. The network
performs pose regression to generate a plant location likelihood map. Our
approach allows us to cope with visual variances of plants both for different
species and different growth stages. We achieve high localization accuracies as
shown in detailed evaluations of a sugar beet cultivation phase. In experiments
with our BoniRob we demonstrate that detections can be robustly reproduced with
centimeter accuracy.
                  <td style="background-color:None">
                    <a href="http://arxiv.org/pdf/1709.04751v1">Download</a>
                  </td>
                </td>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </body>
  </body>
</html>